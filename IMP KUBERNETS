https://infotechys.com/install-a-kubernetes-cluster-on-rhel-9/ == kubernets cluster installation process. take static-ip for nodes.

IMP:
if the developer are changeing the variables or any other environment variables/endpoints, we will get "CrashLoopBackOff" error.
kubectl.awseksdev02 get pods -n dev-kafka | grep -v Running ==> To see the pods who are having the issues(like above)

kubectl.awseksdev02 get deployments -A | grep -i 0/0 ==> to see deployments currents stage

kubectl == To connect to the kubernets cluster. It will check automatically "~/.kube/config/".

kubectl get nodes == This command pulls the data from ".kube/config", To know how many nodes in kubernets cluster.

kubectl version --short == To know the client version.

Kubectl config get-contexts ==> this command will shows the how many clusters are tagged to this dev server. Let connect to your jump server.

Kubectl config current-context ==> it will tell we are in which cluster.

kubectl config use-context arn:aws:eks:us-east-1:8756:cluster/awseksdev02 == to jump to required cluster.

Kubectl get nodes ==> This command will shows the what are the nodes connected to the cluster after logged in by useing above command.

kubectl describe pod <pod-name> -n <namespace> | grep -A 10 "Init Containers"  ==> is the command to check dyntrace is running.

Git branch -a ==> it will shows the all the branches, now we are in the “feature/fre-dev” branch. We will encourage the developer to put the final code in “feature/pre-dev” branch.

kubectl delete pod podname -n roboshop == To delete the pod without configaration file.

kubectl get all -A ==> to see name space with pods

kubectl logs podname -n roboshop == To see logs of a pod

kubectl get rs -n roboshop == To see the replicas

kubectl get deployments -n roboshop == To see the deploymentssets.

kubectl delete deployment nginx-deployment -n roboshop ==> to see deployment

kubectl delete ns roboshop == To delete the Namespace

aws eks list-clusters --output table ==> login to jump machine and switch to cldadmdv account and run this command,it will list created EKS clusetr in the AWS

YAML SYNTAX USAGE TO BUILD RESOURCES IN KUBERNETS
==================================================
apiVersion: == Every resource creates with the api version(v1).
kind: == what kind of resource you are creating.first letter is capital(kind: Namespace)
metadata: == metadata menas below information
  name: == That resource name(note in kind what ever we are giving that name)
  namespace: == whichname space are we creating pods
  labels: == labels are key=value pairs to call other services, labels have limitations in length and specal characters.
  annotatons: == annotations are used to call outside resource(aws secrats), annotations we there is no linit like variables, we can use special charecters as well.

spec: == it means container specifications
- name: == denotes the container name
  image: == denotes which image we are useing
  env:  == it denotes the environment variable for container. these are key=value pairs.
           ex - name: DEMO_GREETING
                value: "Hello from the environment"

            Note: Like this if there are many environment variables we will go for "configMap". This configMap we can use for N(number) of pods.
                  applications when it requires configaration information it will take from "configMap", configMap is nothing but a key=value pairs.
   resources: here we will define how much cpu and mem we required for the container.
              ex:     resources:
                        requests: == Denotes the how much cpu and memory is required while application start
                          cpu: "100m"
                          memory: "68mi"
                        limits: == Denotes the if application is having more load then what extend the resource limits will go.
                          cpu: "280m"
                          memory: "128mi"
=================================================================================

kubectl get namespaces == To see all namespaces in your kubernets

kubectl delete namespace roboshop ==  To delte the namespace.

kubectl create -f namespace.yaml == To create the the service whatever in "namespace.yaml" file.

kubectl delete -f namespace.yaml == To delte the service what ever we menationed in "namespace.yaml" file.

kubectl apply -f namespace.yaml == if resources not created it will create, if you assign any changes it will update.

kubectl get pods -n roboshop == To see the pods under roboshop namespace.

kubectl exec -it hello-pod -n roboshop bash == To login to pod

kubectl exec -it multi-container -c almalinux -n roboshop bash == If our pod is having multiple containers to login specificall almalinux container.

kubectl describe pod  multi-container -n roboshop == To know about the pod in detailed, like labels info, annotations info

kubectl apply --dry-run=client -f 01-cluster-ip.yaml == To validate syntax of the your file. it wont create anything. it is just dry run. This indicates that your YAML file is correctly configured and ready to apply in your Kubernetes cluster.

k9s installation in RHEL8 server
===================================
curl -LO https://github.com/derailed/k9s/releases/latest/download/k9s_Linux_amd64.tar.gz
tar -xvzf k9s_Linux_amd64.tar.gz
mv k9s /usr/local/bin/
chmod +x /usr/local/bin/k9s
k9s version
k9s == To see graphical view for all pods, namespaces etc...

:ns roboshop ==> to see pods in roboshop
===========================================================================================

 eksctl is the official command line from AWS, by useing "eksctl" we can configure production related clusters.

 eksctl create cluster --config-file=eks.yaml == This is the command to create cluster


===========================================================================================
@#### configMap == advantages of the configmap is if the values are changeing, we are not disturbing the pod. mostly for application code change we will go for cicd (or) build process. if there is no application code change we maintain configaration outside of the application code. then we will save lot of time. so we dont maintain environmental variables in docker, we will maintain in kubernets layer as configMap.
directly we can edit configMap and restart the configmap and restart the pods. automatically new values will fetch.the use of configMap is fetch the values dynamically. 

ex:- 
apiVersion: v1
  kind: ConfigMap
  metadata:
  name: devops-config(IT means configMap name)

data:  ==> it means what data we are giving, here we are giving key=pairs.
  course: devops
  trainer: "krishnareddy" (this information we needs to call to pods, where we required)

kubectl apply -f 07.configMap.yaml -n roboshop ==  To create configMap

kubectl get configmap -n roboshop == To see the configMap details.

kubectl describe configmap confimapname -n roboshop == To see what is in configmap.

=========== keep below info under the container to call configMap to any pod ===========
envFrom:
  - configMapRef:
       name: devops-config
===========================

kubectl edit configmap devops-config -n roboshop == To edit the configmap in server it self.
================================================================================================

@###Secrets in Kubernets == secrets and configmap both are same. just name will change. it is like key=value pair.
 Note: - in secrets we needs to give "base64" value.
        To get base64 value. in gitbash shell execute below commands.
        $ echo -n "admin"|base64
          YWRtaW4=
        $ echo -n "admin123"|base64
          YWRtaW4xMjM=

      To decode the above passwords use below commands.
        $ echo -n YWRtaW4xMjM= | base64 --decode
          admin123

ex: 
apiVersion: v1
kind: Secret
metadata: 
  name: devops-secret(It means secret name)

type: Opaque( it means we cont see that)
data:
  username: YWRtaW4=
  password: YWRtaW4xMjM=

kubectl get secrets -n roboshop == To see the secrets in roboshop name space.
=========== keep below info under the container to call configMap to any pod ==============
    envFrom:
      - secretRef:
          name: devops-secret
===========================================================================================

@###Services: Pod is dynamic(that means this minit it will run next minit pod will terminate), Pod ip address are aphemeral they are not perminent. to achive pod to pod communication in kubernets and to expose the pod to outside world pod must attach to the service.

1) we are useing pods but we are not exposing outside world, if you want to expose pods to outside other applications or outside world, we must use services.

2) Load balancing

3) service mesh

Three types of services
=========================
clusterIP -- It is purely internal to kubernets.

NodePort -- Node port you can expose your pod to outside world

Load balancer -- we can expse to outside world.use in AWS,AZURE,GCS.

IMP Notes
==========
1) first we have pod.
2) we need to attach this pod to service. here label will help to select particular pod to respective service. labels should be unique. below once shows the labels uniqueness. so our pod labels should be unique.
  
  project: roboshop
  component: catalogue
  tier: tier-2


1) ClusterIp: cluster ip for the internal purpus.
apiVersion: v1
kind: Service
metadata:
  name: nginx-service

spec:
         == Here if we are not giving anything here that means "cluster-ip"
  selector: (it means  once pod is created this service is going to attach with witch pod, that is nothing but selector, How it select? that is depends on the labels(in Pod), what are the labels we gave in pod, the same labels we needs to give in service also)
    app: nginx
    demo: krishna (in pod syntax what information we have give the same info 
          we needs to give, then only pod will select the respective service)

  ports:
    protocal: TCP
    port: 80 (service port) 
    targetPort: 80 (container Port, that means people hit service port, port number-80, that will pass the request to container port: 80) 

kubectl get services == To displays the all services in the kubernets.  

2) Nodeport:- if you want to expose your pod to outside world.for nodeport we will get cluster ip. so clusterip is subset of nodeport.

 =just add below lines to above synatx to become nodePort====
 spec:
  type: NodePort
  selector:

  nodeport we dont use.if you want completely external go for go for load balancer, if you want to internal go for cluster-ip. if you go for loadbalancer nodeport will come auomatically. nodeport means to expose the ipaddress of worker node. it is not good to use nodeport that is not correct. nodeport is internal concept in loadbalancer, with out node port loadbalancer will nort work.
=====================================

3) Load balancer:- Load balancer will use for cloud providers. for users directly we are not giving the ports ip address before that we are keeping loadbalancers. providing cluster-ip address to user is not a secure thing, so we keep loadbalancer. if user hits the loadbalncer, the request from the loadbalancer will go to any one of the nodeport via port number 31197 for the server, the nodeport request the cluster-ip, that cluster -ip send request to pod.

=just add below lines to above synatx to become Load Balancer====
spec:
  type: LoadBalancer            
  selector:
===========================================================================================
REPLICASET:- replicaset means same of pods. one pod is not enough to bare the load.based on the requests the pods needs to be scale. Pod is subset of replica set. first replica set will create. it will check it self for what we are creating replicas
 replicaset will refers to the labels, then witch pod is having the same labels for that pod. it will create replicas.
  matchlabels:
    app: nginx
    tier: frientend

ex:
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: nginx-rs
  namespace: roboshop
  labels:
    app: nginx
    tier: frontend

spec:
  replicas: 3 #it means three replicas we want
  selector:  # replica-set labels
    matchLabels:
      app: nginx
      tier: frontend
  
  template:   #it is pod defination, it dont have apiVersion: and kind: , the reason pod is subset of the replicaset.
    metadata:
      labels:
        app: nginx
        tier: frontend

    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80

Generally replicaset is for the to create number of replicas for the pod. we are useing application version(ex: cataloghu:1.0.0) is running, in next release we want to run (ex: cataloghu:1.2.0) for that needs to go re-release. in tis case we needs delete the existing one(kubectl delete -f applicationname.yaml) and recreate with the latest code(kubectl apply -f applicationname.yaml).

note: applicationname.yaml file should have upto date code.
====================================================================================================================================================
DEPLOYMENTSET:- ReplicaSet is manually used to maintain number of replicas, but it con't identify the changes in the application and con't update.
 "To identify the changes in the application and to update the version we have deployment".

imp:services(clusterIP < NodePort < LoadBalancer)
     sets(Pod < replicaset < deploymentset)

  ex: syntax same as replicaset just replace "kind: Deployment"

  so in deploymentset application is updated with the single command "kubectl apply -f filename.yaml"

  note: applicationname.yaml file should have upto date code.

  kubectl delete deployment nginx-deployment -n roboshop ==> to see deployment

  kubectl get  deployment -n roboshop == To see the deployments

  How deployment is working?
     old replicaset is creating the new replicaset. once pod in new replica set will up then pod in old replicaset will down, means at a time 3 pods will run because we set responsibulity is to replicaset is at any sitvation it has to run 3-replicas. like this new pods will create in new replicaset. finally replicaset is going to deleted. then new replicaset is going to attach to deploymentset. This process is called rooling update. This is zero down time deployment. but few seconds our application old version and new version both are running.
===========================================================================================
=====================================================================================================================================================================================
ROBOSHOP PROJECT:- for any project below two point are important.

1) image should exist (or) build the image ==> as per the application requirement build the image.
2) configaring image through manifest files ==> It means we are informing to kubernets how to run that image by manifest files.
          
          Here in kubernets first we build the image, that image we are pushing to dockerhub, after that we create kubernetes manifest files and informs how kubernets will run that image.

Q) who pull the images?
A) Kubernets cluster is pulling the images.

Below errors can expect by above two steps
------------------------------------------
image is there or not.
image build properly or not.
what are the image versions.
does image pushed to repositary(Nexus or dockerhub or ECS) or not.
kubernets is having image pull access or not.
once image pulled, then configaration changes(step-2) correct or not.

https://github.com/daws-76s/roboshop-documentation ==> for the project documentation

cat ~/.docker/config.json ==> to check which account is accessing latestly to the docker hub. here credentials are in base64.

docker info == To dispaly all information about docker including above authentication info.
======================
image pull policy: 
imagePullpolicy: Always ==> it means kubentes cluster everytime pulls the latest image from repository(ECR, dockerhub, etc), so always keep "Always" that is recamanded.

imagePullpolicy: ifNotPresent ==> Kubernetes pulls the image only if it is not already present on the node.

imagePullpolicy: Never ==> Kubernetes will never attempt to pull the image. It uses only the local copy.
==========================================================================================

To avoid multiple times giving "-n namespace", we can try below things.

git clone https://github.com/ahmetb/kubectx /opt/kubectx
ln -s /opt/kubectx/kubens /usr/local/bin/kubens
kubens roboshop ==> To move to roboshop namespace
kubectl get pods == for all commands we can execute under same namespace.

==========================================================================================

mongodb port no: 27017

[root@lenlk8smastert01 debug]# kubectl get pods -o wide
NAME                         READY   STATUS    RESTARTS   AGE    IP               NODE               NOMINATED NODE   READINESS GATES
catalogue-78cfc54744-r5l7d   1/1     Running   0          5h6m   192.168.58.241   lenlk8sworkert01   <none>           <none>
debug                        1/1     Running   0          11m    192.168.58.232   lenlk8sworkert01   <none>           <none>
mongodb-d8d59d9db-cv9m2      1/1     Running   0          20h    192.168.58.231   lenlk8sworkert01   <none>           <none>
[root@lenlk8smastert01 debug]#

as per this o/p catalogue is in one node and mongodb is another node etc.

root@mongodb-d8d59d9db-cv9m2:/# netstat - lntp # after login the pods executing this command, here mongodb  is running on port No: 27017
Active Internet connections (w/o servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 mongodb-d8d59d9db:27017 192-168-58-241.ca:52560 ESTABLISHED
Active UNIX domain sockets (w/o servers)
Proto RefCnt Flags       Type       State         I-Node   Path
root@mongodb-d8d59d9db-cv9m2:/#
 

mysql works == 3306 port number

mysql -u shipping -pRoboShop@1 == After login to mysql pod, again login to mysql server
mysql> show databases; == It will displays the databases
mysql> use cities;
Database changed
mysql> show tables;
==============================================================================================

Q) What is statefull applications?
A) we have data in some applications those are called statefull applications, where the data is crucial..(or) applications where the storage is mandatary.

 below are examples.

    MONGODB --> when ever we are creating some users that stores in mongodb and redis.
    RABBIT MQ--> Whenever we are creating some order that will store in rabbitmq.
    MYSQL --> shipping related information will come from mysql.
    MONGODB--> Cataloghu where the product information will shared.

Q) What is stateless applications?
A) Applications where the storage is not mandatary, they are called stateless applications.
 ex:- catalog, cart, user and shipping etc.

Q) In kubernets server where the data is store?
A) The data is getting stored in EC2 worker nodes.

Q) we clearly pods are ephemeral(keep on changeing), so in kubernets "ec2 nodes are perminent or temparary?
A) Nodes are ephemeral, so if you start storing the crusial data in "ec2-worker' nodes, those nodes tomorrow may deleted. then we will
  loose data again. so you should not take statefull applications storing the data in worker nodes. in that case we should have some
  external storage. we should mount the 'storage(EBS/EFS)' to kubernets cluster. 

  The storage can be EBS/EFS.

  EBS --> is like HARD disk.
  EFS --> It is like NFS storage.

STORE THE DATA IN WORKER NODES (OR) KUBERNETS VOLUMES:
-----------------------------------------------------
1. EmptyDir
2. HostPath    --> these two things will use to store the data in worker node or internal volumes. this data is not perminent but we 
   have some use cases.

Q) What is EmptyDir in kubernets?
A) In sidecar concept we have two containers in side the pod. pod characterstics are multiple containers inside the pod share same 
    network and storage. one container is serving for the application and another container shifting the logs to ELK. 

    EmptyDir: emptyDir is used for sidecar pattern, we mount volume to the Sidecar and main container useing EmptyDir.as the name says this emptyDir volume is intially empty. All the containers in pod can read and write the same files in the emptyDir volume. when you create the emptyDir it will create the volume inside the node and it will give access to pod.

    EmptyDir is used in sidecar pattern to access the main container logs to ship it to the external logging system like elk.

     then sidecar get access to storage, so sidecar project have filebeat. filebeat is the public image for the for elastic search, we put filebeat in sidecar, this filebeat access the logs and ship. there is some configaration for the filebeat.

    i/p and 0/p --> input is for the prod logs to success, o/p is nothing but where to shift the logs.

    we have to inform filebeat, we can take filebeat images from the internet but we needs to give this configaration where to send the logs.

Q) Pods are aphermal so the pod may terminate do we have logs avilable?
A) we dont have logs if the pods are temparary. we will shift the logs to elastic search, but who will ship?
    Main pod responsebulity is sending response to requests, then we will keep sidecar, the purpus of the sidecar is it will access the
    storage and contineously shift logs to the ELK. That is usage of the sidecar.
 
 
Q) What is HOSTPATH?
A) In Ec2 /var/log/directory we have lot of logs, like pod logs these logs also very important to understand what is happening underline worker node. like to know worker node is connected to kubernets master? is anyone hacking the underlying nodes, how is 
monitaring for the underlying nodes, How is the cpu utilization these all things are important, so we needs to ship all those worker 
node logs to external system. How many worker nodes we have those many logs we needs to ship.

  Giving host path to specific directory, it is purely adminstrative task and you should give only read-only access. if you give write access pod can do anything in the host. Hostpath is dangerous but it can had a valid reason to ship the underline worker node logs to
  external system, we can configure host path with some precations like configaring a particular directory and as well as only read-only access. HERE DAEMONSET PLAYS wider role.

Q) How to write node-port is read-only?
A) we will declare in manifest file under volume mount.

    volumeMounts:
    - mountPath: /foo
      name: example-volume
      readOnly: True --> here we should give readly

Q) What is DaemonSet?
A) DaemonSet will make sure a pods runs in each and every worker node. if you delete one worker node that will gone automaticaly. if you add new node to the cluster a daemonset will automatically create the in that worker node. FLUENTD(From elk) will be deployed as a daemonset that can access the underlaying worker node logs and send them to elk. FLUENTED contains access to the /var/log directoryand shifts the logs to external system.
              How many worker nodes are there those many pods will created by daemonset without specifying the number. if you delete daeemonset all pods in worker node will be deleted.

Q) Uasges of DaemonSet?
A) 1) To configure storage to every node(Running a cluster storage daemon on every node)

   2) To push logs from every worker node( running a log collection daemon on every node)

   3) To Set monitaring to every node (Running a node monitaring daemon on every node)

ekctl delete cluster --config-file=eks.yaml --> To delete the cluster after the practice.


 Q) In one namespace we are deploying the all pods. in real time different namespaces we have then how?
    (or) WEB is in one namespace and APP is in another namespace, database is in different namespaces, then internally how they communicate?

  A) There is a communication between namespace to namespace. then service change bit different.
     with in namespace( web, app, Db) if we give service name that will connect. if there are different name spaces then.

     syntax: different namespace.svc.cluster.local
=============================================================================

EXTERNAL VOLUMES
----------------

external volumes like HARD DISK and CLOUD DRIVERS.

HARD DISK--> Near to computer as near as possible.

CLOUD DRIVERS--> Cloud drivers are network drivers.. like NFS..

if you store the data inside kubernets cluster or worker nodes that is defnetly not reliable, it may loose the data anytime. so we desided keep the storage outside.

IMP NOTE: "This Storage we needs to connect with cluster"

Generally to manage storage we required storage team, in cloud there is no big responsibulity. heavy lifting is to handle by AWS(or) cloud platforms. still kubernets adminstarter should know the storage concepts.

HERE we manage the storage by "STATIC PROVISIONING AND DYNAMIC PROVISIONING"

STATIC PROVISIONING
-------------------

Static provisioning means we needs to create the storage manually. it is tuff to add disks and sync the data, but in cloud that is simple.

In static provisioning we have two types of volumes. they are EBS and EFS.

EBS -- EBS is nothing but Elastic block storagelike HARD DISK.

EFS -- EFS is nothing but ELASTIC FILE SYSTEM, it is like in NFS.

EBS(ELASTIC BLOCK STORAGE)
--------------------------
1) first we needs to create the storage, either storage admin or k8 admin will create the storage. our goal is we needs to store the data in outside, that outside is EBS.

Creating the EBS VOLUME
AWS--> ELASTIC BLOCK STORAGE --> VOLMES --> CREATE VOLUMES

VOLUME SETTINGS
volume type(General purpose SSD(gp3))
size(GIB)(100)
IOPS(3000)
avilable zone(us-east-1a)
snapshot ID - Optional( Dont create the volume from a snapshot)
                      create volume(enter)

CONDITION:- our servers are in which avilabulity zone, that avilabulity zone only we needs to creates the volumes.

Q) Can we create the volume in "us-east-1c"?

A) Availabulity zone is like datacenter where all servers to be present. so where the server is avilable there only we needs to create the EBS volume.

NOTE-1:- if server is avilable in "US-EAST-1B" and i created volume in "US-EAST-1C" that will not work in EBS.

NOTE-2:- We needs to make this EBS volume avilable for the "k8-cluster", for this we should install the "aws-ebs-csi" drivers should be installed( like to put pendrive or keyboard to laptop to detect it requires the drivers). for kubernets cluster external storages(EBS?EFS) to attach we should install the drivers.

Server:- kubectl apply -k "github.com/kubernetes-sigs/aws-ebs-csi-driver/deploy/kubernetes/overlays/stable/?ref=release-1.28" -- installing the driver.

NOTE-3:- EC2 instance and EBS both are different resources. so a proper role shouuld be attached to 'EC2' instance to access 'EBS'.
          To create EBS volume, To mount EBS volume and describe EBS volume , delete EBS volume..all these roles should have to EC2 instance. the reason is that they are different rosources.

AWS--> EC2 INSTANCE --> Security --> IAM Role(eksctl-roboshop-node group or any role) this role is attached to ec2 instance, this role all ready have some permissions by default but also add few more permissions.

Permissions --> Add permissions --> attach policy
other permissions policy(ec2 full access{search this})--> select Amazon EC2 full access( select this after this instance can create and delete volume everything)

ADD PERMISSIONS(Enter)

KUBERNETS VOLUME OBJECTS
------------------------
we have few volume related objects, how we have Pod,ConfigMap,Secrets, resource like for volume also we have few resource.

Kubernets is creating the persistence volume, this persistence volume will repracent the storage.

PERSISTENCE VOLUME(PV) --> persistence volume will repracent the storage, why it needs to repracent?
A) AS a kubernets adminstrater we dont have access to the EBS storage. Persistence volume work as a wrapper. if we do operations on the persistance volume kubernets in background do operations on the storage. So pv repracents the physical volume external to kubernets.

PERSISTENCE VOLUME CLAIM(PVC) --> claim means we needs to request something. so pods should request volume through PVC(Persistance volume claim). PVC is not like a physical repracentaion it is just like a request to the PV.
            To use EBS Volumes first we should create the 'PV', it is exactly equal to the EBS and then we have to claim pvc. pvc is like a claim.

IMP NOTE:- POD connect to the pvc and pvc connect to the PV.

Q) When you apply where your pod is getting created?
A) Any Worker node.

Q) Is there any chance this POD may goes to 'us-east-1c'zone worker node?
A) yes there are chance this pod may goes to 'us-east-1c' zone. remaining probabulity it may goes to "us-east-1b" so we have to       control  this thing, it will control by 'kubecontroler'component in kubernets. which worker node the pod goes to mount that volume.
other wise if the pod created in another "us-east-1c" volume not mount. we have something called nodeselectors. to assign pod to node we needs to label the nodes.

kubectl get pods -o wide ==> To see which worker node in which avilabulity zone.

select server which is in 'us-east-1b' and check private "IPV4" address is 192.168.43.218

kubectl label nodes ip-192-168--43-218.ec2.internal zone=1b
kubectl label nodes ip-192-168-53-69.ec2.internal zone=1b

in kubernets manifest file we needs to update below info.

nodeselector:
  zone: 1b   --> Give this info in above pod defination

while POD is creating it will check the node selector zone "1b" which node is labeled as "1b" in that server only it will create. This is how you can control the pods in the worker node.

Node selector is the basic thing to go the pod to particular node. if you dont want to go we have few NODE affinity and Node anti offinity.

VOLUME STATE
------------
AWS --> EC2 instance --> Volumes(Here we can see volumes)

if volume state is "Avilable" this is not attached.
if volume state is "in-use" that means that volume is successfully attached to pod and used.
===================================================================================================

DYNAMIC PROVISIONING
--------------------
DYNAMIC provisioning means volume should be created automatically. it means kubernets should be created volume automatically.Like statics provisioning we required "pv" and "PVC". those objects created by the "storage class" dynamically baesed on the request.

if we created volume manually we needs to create the 'PV' manually. when the volume is automatically created by the "storage class", then "PV" also automatically created by the storage class.

NOTE-2:- We needs to make this EBS volume avilable for the "k8-cluster", for this we should install the "aws-ebs-csi" drivers should be installed( like to put pendrive or keyboard to laptop to detect it requires the drivers). for kubernets cluster external storages(EBS?EFS) to attach we should install the drivers.

Server:- kubectl apply -k "github.com/kubernetes-sigs/aws-ebs-csi-driver/deploy/kubernetes/overlays/stable/?ref=release-1.28" -- installing the driver.

NOTE-3:- EC2 instance and EBS both are different resources. so a proper role shouuld be attached to 'EC2' instance to access 'EBS'.
          To create EBS volume, To mount EBS volume and describe EBS volume , delete EBS volume..all these roles should have to EC2 instance. the reason is that they are different rosources.
================================================================================================

kubectl api-resource --> it will display resources in kubernets. there are two types of resource.

1) VPC based resource
2) Non-VPC based resource

TIP:- If any service is having the network that is called the vpc based network.

ex:- EC2 instance and route-53 all are vpc based resource.

if there is no networking that resource is called non-vpc based network.

like this in kubernets there are two types of resource

1) Cluster name space

2) cluster non-name space

kubectl api-resource (namespaced {true})--> to check cluster name space or cluster non-name space.

Q) POd is namespace resource (or) Non namespace resource?

A) if resource are created with in the namespace those resource are called name-space resource.

  kubectl api-resource (namespaced {true}) --> that means those resource are called namespace resource.

  in cluster level also we will create the resource(storage) those resource are called cluster non-name space resource. this storage class is created by the adminstarter.

  EBS will have one storage class clusterwise, for the entire kubernets cluster adminstarater strater creates the storage class.

  kubectl get sc (name (ebs-sc)) --is storage class created by the adminstarter and informed to all engineers like we have created the for the dynamic provisioning. then projects can create the volume dynamically according to the requirement.

  persistance namespace(PV) is also not name space resource.
  persistance namespace claim(PVC) is name space resource.

kubectl get pv(observe the o/p here)
kubectl get pvc (Here observe the o/p and volume) ==here we came to know which pv(persistent volume) it is created.

Q) What is the reclaim policy(delete) from above commands o/p?
A) RECLAIM POLICY--Delete means if we delete the pod automatically underline volume also deleted.
   RECLAIM Policy-- Retain means if we delete the pod then volume is avilable.
                 if you want to change the RECLAIM POLICY from DELTE to RETAIN we have to update in storag class.

===============================================================================================================

EFS (Elastic file system) -- EFS just like a NFS/google drive. it is somewhere in networks.we can work on the storage based on NFS.

AWS -- search for (EFS) --> Create file system.

Name-optional(efs-static)
virtual private clour(vpc-odoee47e) -- here we have doubt which vpc we needs to give so selectserver -- details --vpcid(copy this) and click on your vpc and compare this.

create(enter) -- click here to create the EFS.

once created click on (name {efs-statisticks}) --> Networks (here we see security group, it is vpc default security group{take note})

security group (search bar {paste aove security group}) -- here we got o/p, that security group is connected to security group.

To add this EFS TO NFS, ee nfs ki connect ina security group lo

vpc --> security groups --> sg00091(EFS connected sg) --> EDIT inbound rules --> add rule (NFS TCP 2049 custom sge8470) here we nneds to allow security group which were assigned instance.

select EC2 instance --> security --> security groups(sgeg8470) -- here the meaning is port 2049 is opening that to connect to our instance 

SAVE RULES( TO save all rules)

IMP NOTE: There is no size limit for the EFS file system. it will keep extending based on utilization.

we can store upto 47.9 tb file. this is based on the NFSv4.0 (network file share), how much space we use that much cost we needs to pay.

3) INSTALL EFS CSIDRIVERS
-------------------------
SERVER
------
cd
kubectl kustomize "https://github.com/kubernetes-sigs/aws-efs-csi-driver/deploy/kubernetes/overlays/stable/ecr/?ref=release-1.7" > private-ecr-driver.yaml
kuectl apply -f private-ecr-driver.yaml --> drivers will install

4) CREATE PV
------------
google:- https://github.com/kubernetes-sigs/aws-efs-csi-driver/blob/master/examples/kubernetes/static_provisioning/specs/pv.yaml --> here we will see reference code to create the PV.

 EFS STATIC
  ----------
  1) CREATE EFS FILE SYSTEM
  2) EDIT THE SECURITY GROUP SO THAT IT WILL ALLOW PORT 2049 FROM WORKER NODE
  3) WE SHOULD HAVE DRIVERS INSTALLED(SEE ABOVE)
  4) GIVE EC2 ROLES EFS FULL ACCESS
  5) CREATE PV
  6) CREATE PVC
  7) USE PVC IN THE POD
================================================================
EFS DYNAMIC
-----------
TO-do efs dynamic we should have storage class
google:- https://github.com/kubernetes-sigs/aws-efs-csi-driver/blob/master/examples/kubernetes/dynamic_provisioning/specs/storageclass.yaml -- reference code for EFS Dynamic.

kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: efs-sc
provisioner: efs.csi.aws.com
parameters:
  provisioningMode: efs-ap
  fileSystemId: fs-92107410
  directoryPerms: "700"
  gidRangeStart: "1000" # optional
  gidRangeEnd: "2000" # optional
  basePath: "/roboshop" # optional --> it is created in AWS for access point. like this if amazon and flipkart came in a cluster, like this we needs to create another another storage class for that project. like projectwise we will create the storage class. After this we can inform your project engineers we have roboshop storage class created you can create volumes.

  IMP NOTE:- HERE dynamic EFS means not EFS storage created, that means dynamic points which are in EFS will create dynamically.

  AWS --> amazon efs --> file systems --> select one file system(used for this project)--> Access points(Not showing any access points)

  TRoubleshooting
  -----------------
  check PV created, because PV is physical repracent

  kubectl get pv(efs-static--> Pv is created)

  kubectl get pods --> check pods are created 

  kubectl describe pod podname( check events here)

  kubectl get pvc( status is pending ) --> issue is here

  kubectl describe pvc efs-dynamics --> Access is denied please ensure you have right AWS permissions.

  AWS -- select the ec2 server--> security --> IAM role(click here) it will route you to

  IAM --> roles --> eksctl --> roboshop -node group --> Permissions --> add permissions --> attach policy --> SELECT (Amazon elastic file system full access)

  ADD permissions(done)

  now delete and recreate the deployment

  AWS --> amazon efs --> file systems --> select one file system(used for this project)--> Access points ( here we can see access point id and path ) this access point is only for the roboshop project, like this many projects we can see access point created.

=======================================================================================================================================

HELM CHARTS
-----------
HELM charts are used for the TWO purpose.

1. Tempalates the kubernets manifest file/ To parameterise the kubernets manifest file.
2. Package manager for kubernets.

1) Tempalates the kubernets manifest
------------------------------------
There are two steps for kubernets.

1) Build the image / Pull the image if it is public.
2) Configure the image through manifest file to run pod.

Q) Is there any frequant changes in manifest file?

AQ) ONLY FEW changes, what are those few changes?

A) May be repelica count, IMAGE version is keep on changeing. Today if the image version v1 release, tomorrow v2 release, later two weeks v3 release etc...... if you take service nothing will change. take deployment only image will change and annotations and replicas will change.

if you take ConfigMap..... Below things are hardly changeing.

Data:
  REDIS_HOST: "redis"
  CATALOGHU_HOST: "cataloghu"
  CATALOGHU_PORT: "8080"

Once kubernets manifest files ready changes are very rare. Those changes we needs to perform automatics, no needs to edit the manifest files, automatically we needs to perform.

if there is modification in main developemt or main configration file if we do changes by mistake we may edit in other lines, so in that case we convert them as a variable and pass that variable from outside.

what this helm do keep all constarnt values separate and changeing values separate ga maintain chastundhi. That is helms one of the advantage.

HELM INSTALLATION
-----------------
server:-
$ kubectl get pods --> to see the pods
$ curl -fsSL -o get-helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 --> To install helm
$chmod 700 get-helm.sh
$./get-helm.sh
$ helm version {o/p version.BuildInfo{version: "v3.14.2"}}

HELM CHART STRUCTURE
--------------------
HELM chart structure we should discuss.

chart.yaml --> it is first and mandatary file, in this file which one is required which are not required we will check.
apiVersion: v1 #required
name: nginx # writing helm chart for nginx, then name is nginx(required)
version: 0.0.1 # Your helm chart version
descritption: This is customized helm chart for nginx
appVersion: 1.0.0 #app version and chart version both are different.

TEMPLATES --> templates folder is mandatary, in this templates folder we are going to keep what are the kubernets resource use we are going to keep here. in this folder we are going to keep "deployment.yaml" and "service.yaml" files

IMP: chart.yaml templates(deployment.yaml, service.yaml) --> These two are mandatary files.

clone github repo to the server then
$cd helm-charts
$ls(chart.yaml, Templates) --> these two are mandatary files.
$helm install nginx(any name we can give) . --> here HELM command is checking 'chart.yaml' in current folder, just like docker build.
$helm list --> it will shows the deployed one. which one deploy?

A) what helm do by default it will search for 'templates' folder in this folder what are the manifest files are there in which in background automatically "kubectl apply" ani istundhi.

HERE HELM usage is ....few values will change(repelicas, annotations and image versions).. these change values we needs to maintain "values.yaml" file in "helm" root folder.

values.yaml file -- here helm goes and fetch required values.

we will call "values.yaml" file values to "deployment.yaml" file like below

replicas:{{.values.deployment.replicas}}

ex: image version

deployment.yaml
image: "nginx:{{.values.deployment.imageVersion}}"

                 .values means it is searching "values.yaml' and taking the deployment image version.

CONSIDER:- we are upgrding the latest version. in that case we are not touching the manifest files and we update info in "values.yaml" file, then go for deployment.

server; pull the git repo latest data
$helm upgrade nginx . --> is use for upgrade, while we are going for upgarde best practice is 'chart.yaml(version:0.0.2 #this is chart version) version change chayavachu.

if we are going for release chart version and application both needs to change.

$kubectl get pods --> here application is upgrade automatically. age is 15 sec...etc..
                   so helm is used to parametarise the kubernets manifest files, here the main files are "chart.yaml" "values.yaml" and templates(service.yaml, deployment.yaml) directory..

2) HELM IS USED PACKAGE MANAGER FOR KUBERNETS:
----------------------------------------------
Q) VM's are getting packages from servers and yum is installing it, what about the kubernets?

A)if it comes to kubernets either build the image or pull the image.

HELM advantages

1) Image is already in public.
2) helm have some public repos to configure the image through manifest files.
 
 For EBS we took EBS drivers in public and install by useing below command in server level.

kubectl apply -k "github.com/kubernetes-sigs/aws-ebs-csi-driver/deploy/kubernetes/overlays/stable/?ref=release-1.28" --> this command takes the yaml files from internet and install it.

These yaml files we can install it by helm, to do follow below process.

1) ADD helm repo(in this repo we have kubernets resource files)
helm repo add aws-ebs-csi-driver https://kubernetes-sigs.github.io/aws-ebs-csi-driver

helm repo list --> we will get all repos

helm repo update --> latest ga eamina untey idhi thesukoni vastundhi

helm upgrade --install aws-ebs-csi-driver aws-ebs-csi-driver/aws-ebs-csi-driver --namespace kube-system --create-namespace --> to install the drivers

helm list -n kube-system --> it will diplays the installed drivers

helm uninstall aws-ebs-csi-driver -n kube-system --> to uninstall the drivers.

helm history nginx--> to see how many times we install for the specific application. here we see versions and do rool backs easily?

Q) when we go for rool backs?

A) once we go for new release of the applications, that application is suddenly failed then we will go for roll back. how fast we are doing rool back that is important. with in minute if you do rool back you do not loose the business.

helm rollback nginx --> automatically it went one version back.
kubectl get pods --> all pods are in running state, roll back done.

helm history nginx
o/p
1 install completed
2 upgrade completed
3 roolback to 1 --> means rool back to first version.

After one year if you see 100 releases, if you want to go which release that relase we will go.

helm rollback nginx 2 --> it means going to 2 release.

kubectl get pods --> to check the pods running status.

helm show values --> it will shows the 'values.yaml' file.

HELM works very effective way in cicd pipeline.

helm upgrade nginx --set deployment.replicas=10 --> replicas will increase to 10

kubectl get pods --> we will see 10 pods here.
=====================================================================================================================================














          










